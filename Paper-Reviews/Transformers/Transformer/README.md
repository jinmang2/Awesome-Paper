# Paper-Review

#### 2020.03.30 Attention Is All You Need

- 논문 제목: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- 작성자: [MyungHoon-Jin](https://github.com/jinmang2)
- 리뷰 링크:

#### 선행되야 하는 개념들
- RNNencdec
- Attention Mechanism
- Self-attention
- Residual connection
- LSTM
- Layer normalization
- Dropout
- Label Smoothing
- seq2seq
- MemN2N
- Extended Neural GPU, ByteNet, ConvS2S
- transduction model
